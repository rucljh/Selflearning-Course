[TOC]



****

<center><span style='font-size:39px'><strong>机器学习笔记</strong></span></center>

# 决策树(Decision Tree)

分类数据

## ID3算法

在每个结点处选取能获得最高信息增益的分支属性进行分裂，分裂目的是将整个决策树的样本纯度提升。

信息熵衡量样本集合纯度。
$$
信息熵Entropy(S)=-\sum_{i=1}^{m}p_i\log_2(p_i),p_i=\frac{|C_i|}{n}
$$
若分裂后的样本集纯度提高，则样本集的熵值会降低，降低的量即为该分裂方法的信息增益。
$$
信息增益
Gain(Y)=Entropy(S)-Entropy(S|Y)\\
Gain(Y)=Entropy(S)-\sum_{i=1}^{\nu}\frac{|S_i|}{|S|}Entropy(S_i)
$$
选取信息增益最大的分类属性为当前结点。

分裂终止条件：末端结点最小样本数小于设定值，或决策树达到预先设定的深度。

## C4.5算法

ID3算法用信息增益作为度量，而C4.5算法用信息增益率作为度量，弥补当属性分类数$\nu$较多时，信息增益相对较大的问题。

$$
Gain\_ratio(Y)=\frac{Gain(Y)}{Entropy(Y)}\\
Gain\_ratio(Y)=\frac{Gain(Y)}{-\sum_{i=1}^{\nu}\frac{|S_i}{|S|}\log_2\frac{|S_i|}{|S|}}
$$
选取信息增益率最大的分类属性为当前结点。

## Cart算法

二分循环分割方法，生成二叉树。如果某分支属性有多于两个取值，分裂时选择属性值最佳组合进行分枝。假设某属性存在$q$个可能取值，那么该属性的可能分裂方法有$2^{q-1}-1$种。

在分枝处理中，分支属性的度量指标是Gini指标。
$$
Gini(S)=1-\sum_{i=1}^mp_i^2,p_i=\frac{|C_i|}{|S_i|}
$$

$$
Gini(Y)=\frac{|S_1|}{|S|}Gini(S_1)+\frac{|S_2|}{|S|}Gini(S_2)
$$

选取Gini指标最小的分类属性为当前结点。

ID3和C4.5（小样本）只能做分类，而CART（大样本）可以做分类和回归。

## 连续属性离散化

### 非监督离散化

等宽离散化：划分为宽度一致的若干个区间。

等频离散化：划分为数量相等的若干个区间。

聚类离散化：用聚类方法划分为不同的簇。

### 监督离散化

有时能产生比非监督离散化更好地结果，常通过选取极大化区间纯度的临界值来进行划分，如C4.5、Cart算法中的连续属性离散化。

## 过拟合问题

### 检验是否存在过拟合

训练误差代表分类方法对于现有训练样本集的拟合程度。

泛化误差代表此方法的泛化能力，即对于新的样本数据的分类能力。

模型的训练误差较高，则称此分类模型欠拟合。

模型的训练误差较低，但泛化误差较高，则称此分类模型过拟合。

对于欠拟合问题，可以通过增加分类属性的数量、选取合适的分类属性等方法，提高模型对于训练样本的拟合程度。

对于样本集，常用70%:30%比例划分为训练样本集和测试样本集。

决策树误差曲线可以用来描述训练误差和泛化误差随决策树生长的变化情况。

### 解决过拟合问题

注意训练集的质量，选取具有代表性样本的训练集；避免决策树过度增长，通过限制树的深度来减少数据中的噪声对于决策树构建的影响，一般采取剪枝的方法。

剪枝可缩小决策树的规模，降低最终算法的复杂度并提高预测准确度。

预剪枝：事先设定终止条件，在形成完全拟合训练集的决策树之前终止迭代，避免决策树规模过大而产生过拟合。

后剪枝：先让决策树完全生长，之后对子树进行判断，用叶子结点或子树中最常用的分支替换子树，不断改进决策树，直至无法改进为止。

错误率降低剪枝(Reduce Error Pruning, REP)是后剪枝策略中最简单的算法之一。从叶子结点向上，依次将决策树的所有子树用其样本中最多的类替换使用一个测试集进行测试，记录下对于决策树的每棵子树剪枝前后的误差数之差，选取误差数减少最小的子树进行剪枝，将其用子样本集中最多的类替换。适用于大样本集。

## 分类效果评价

二分类问题，可能出现的分类情况

|            | 检测为阳性 | 检测为阴性 |
| :--------: | :--------: | :--------: |
| 实际为阳性 |     TP     |     FN     |
| 实际为阴性 |     FP     |     TN     |

1. 准确率：在所有样本中，被正确分类的样本数所占比例
   $$
   accuracy=\frac{TP+TN}{TP+FN+FP+TN}
   $$

2. 精确率/误报率：在被判断为阳性的样本中，实际为阳性的样本所占的比例
   $$
   accuracy=\frac{TP}{TP+FP}
   $$

3. 召回率/查全率/真阳率(TPR)：在实际为阳性的样本中，被判断为阳性的样本所占的比例
   $$
   recall=\frac{TP}{TP+FN}
   $$

4. $F$值为精确率和召回率的调和平均
   $$
   F=\frac{(\alpha^2+1)\times accuracy\times recall}{\alpha^2(accuracy+recall)}
   $$
   $\alpha$为调和参数，当$\alpha$取值为1时，$F$值就是常见的$F_1$值
   $$
   F_1=\frac{2\times accuracy\times recall}{accuracy+recall}
   $$

5. 伪阳率：在实际为阴性的样本中，被判断为阳性的样本所占的比例
   $$
   FPR=\frac{FP}{FP+TN}
   $$

6. 接受者操作特性曲线(Receiver Operating Characteristic, ROC)，以伪阳率(FPR)为横坐标，以真阳率(TPR)为纵坐标绘制的曲线图。曲线下的面积称为AUC(Area Under Curve)，AUC值越大，表示分类模型的预测准确率越高，ROC曲线越光滑，一般代表过拟合现象越轻

7. 保留法：将数据集按比例划分为训练集和检验集，两个集合中样本随机分配且不重叠

8. 蒙特卡洛交叉验证/重复随机二次采样验证：随机将数据集划分为训练集和检验集，使用检验集检验训练集训练的模型效果，多次重复此过程，取平均值作为模型好坏的评价标准。也可视为多次进行保留法

9. k折交叉验证法：将样本集随机地划分为k个大小相等的子集，在每一轮交叉验证中，选择一个子集作为检验集，其余子集作为训练集，重复k轮，保证每子集都作为检验集出现，用k轮检验结果取平均值作为评价标准。最常用的为十折交叉验证

# 集成学习(Ensemble learning)

用多种学习方法的组合来获取比原方法更优的结果。采用弱学习算法，即分类正确率仅比随机猜测略高地学习算法，但是组合之后的效果可能高于强学习算法，即集成之后的算法准确率和效率都很高。

## 装袋法(Bagging, or Bootstrap Aggregating)

组合多个训练集的分类结果来提升分类效果。从数据集中多次有放回地采样获得多个大小相等的训练集，由此建立多个分类模型，组合各模型的分类结果作为最终结果。

装袋法由于多次采样，每个样本被选中的概率相同，因此噪声数据的影响下降，所以装袋法不太容易受到过拟合的影响。

装袋法对分类效果的提升程度与原模型的结构和数据质量有关。若分类回归树的树高度设置为3或5，原模型的分类效果较好，提升空间也就小。

## 提升法(Boosting)

提升法与装袋法相比每次的训练样本均为同一组，并且引入了权重的概念，给每个单独的训练样本都会分配一个相同的初始权重。然后进行T轮训练，每一轮中使用一个分类方法训练出一个分类模型，使用此分类模型对所有样本进行分类并更新所有样本的权重：分类正确的样本权重降低，分类错误的样本权重增加，从而达到更改样本分布的目的。由此，每一轮训练都会生成一个分类模型，而每次生成的这个分类模型都会更加注意之前分类错误的样本，从而提升分类样本的准确率。对于新的样本，将T轮训练出的T个分类模型得出的预测结果加权平均，即可得出最终的预测结果。

在提升法中，有两个主要问题需要解决:

- 如何在每轮算法结束之后根据分类情况更新样本的权重
- 如何组合每一轮算法产生的分类模型得出预测结果。

## 随机森林

随机森林是专为决策树分类器设计的集成方式，是装袋法的一中扩展。与装袋法采取相同的抽样方式。装袋法中的决策树每次从所有属性中选取最优属性进行分枝，而随机森林每次从所有属性中随机抽取t个属性，再选取其中最优属性进行分枝，使模型的随机性更强，从而使模型的泛化能力更强。

其中参数t的选取决定了模型的随机性，若样本属性共有M个，通常t取小于$\log_2(M+1)$的最大整数。通常使用的弱分类树用CART算法。

## 梯度提升决策树(GBDT)原理

### 概述

Gradient Boosting Decision Tree中的决策树为回归树。

损失函数：最大熵（分类树）VS均方差（回归树）

把所有树的结论累加起来做最终结论。每一棵树学的是之前所有树结论和得残差（损失函数的负梯度值），这个残差就是一个加上预测值后能得到真实值的累加量。每一步的残差计算相当于变相增大了分错的样本的权重，而已经分对的样本权重都趋于0，使得后面的树能越来越专注于那些前面被分错的样本。

### 特点

超参比较多，多用交叉验证选择最佳参数；

非线性变换比较多，表达能力强，不需要做复杂的特征工程和特征变换；

Boost是串行过程，计算复杂度高，不适合高维稀疏特征；

异常值较多时，可将平方损失用绝对损失$L(y,F)=|y-F|$或Huber损失$L(y,F)=\begin{cases}\frac{1}{2}(y-F)^2 & |y-F|\leq\delta\\\delta(|y-F|-\frac{\delta}{2} & |y-F|>\delta\end{cases}$代替；

### 应用场景

- 线性/非线性回归问题
- 推荐系统

# 支持向量机(SVM)

Support Vector Machine属于有监督学习模型，主要用于二元分类问题，对于多元分类可分解为多个二元分类问题，再进行分类，主要应用场景有图像分类、文本分类、面部识别和垃圾邮件检测等领域。

## 原理

支持向量机在高维或无限维空间中构造超平面或超平面集合，将有限维空间映射到维数高得多的空间中，在该空间中进行分离可能更容易。可以同时最小化经验误差和最大化集合边缘区，也被称为最大间隔分类器。分类边界距离最近的训练数据点越远越好，可以缩小分类器的泛化误差。

在2维2元分类中，找到最优分类数据的分界线，使得对样本数据的分类效果更好的方法就是要尽可能地远离两类数据点，即数据集的边缘点到分界线的距离最大。穿过边缘点的线称作支持向量，分类间隔为2d。











